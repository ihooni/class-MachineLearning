{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15DfB2gLFLgVMf9I02bRkXt-AnY9QU70J",
      "authorship_tag": "ABX9TyPoMbSkq392/+dRYT1cWWvk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1fQp1oNPVtY",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kcdwMnBPXmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "DATA_PATH = './data-nonlinear.txt'\n",
        "\n",
        "# load data from file\n",
        "data = np.genfromtxt(DATA_PATH, delimiter=',')\n",
        "\n",
        "# separate features from labels\n",
        "X_raw = data[:, 0:-1]\n",
        "Y_raw = data[:, -1].reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVr9Ug8YPjlG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Precompute X data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhIZfxVFPmUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = np.zeros((X_raw.shape[0], 1))\n",
        "\n",
        "for i in range(10):\n",
        "  for j in range(10):\n",
        "    new_column = ((X_raw[:, 0] ** i) * (X_raw[:, 1] ** j)).reshape(-1, 1)\n",
        "    X_data = np.concatenate((X_data, new_column), axis=1)\n",
        "\n",
        "X_data = X_data[:, 1:]\n",
        "Y_data = Y_raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gomA17djgKzw",
        "colab_type": "text"
      },
      "source": [
        "## 3. Define functions for logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVowqLQPgMBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic(theta, X):\n",
        "  z = np.matmul(X, theta.T)\n",
        "  Y_hat = 1 / (1 + np.exp(-z))\n",
        "  return Y_hat\n",
        "\n",
        "def objective(Y_hat, Y, lam=0, theta=None):\n",
        "  epsilon = 1e-8\n",
        "  data_fidelity_term = -1 * np.mean(\n",
        "      Y * np.log(Y_hat + epsilon) + (1 - Y) * np.log(1 - Y_hat + epsilon), axis=0\n",
        "  )\n",
        "\n",
        "  regular_term = 0\n",
        "  if lam is not 0:\n",
        "    regular_term = (lam / 2) * np.sum(theta ** 2)\n",
        "\n",
        "  return data_fidelity_term + regular_term\n",
        "\n",
        "def gradient(X, Y_hat, Y, lam=0, theta=None):\n",
        "  m = len(X)\n",
        "\n",
        "  if lam is 0:\n",
        "    return (1 / m) * np.matmul((Y_hat - Y).T, X)\n",
        "  else:\n",
        "    return (1 / m) * np.matmul((Y_hat - Y).T, X) + lam * theta\n",
        "\n",
        "def accuracy(Y_hat, Y):\n",
        "  answer = (Y_hat >= 0.5).astype('float64')\n",
        "  return np.mean(answer == Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmyVHDfW8BgF",
        "colab_type": "text"
      },
      "source": [
        "## 4. Learning with the gradient descent algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeEkdsugCcYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_logistic_regression_model(X_train, Y_train, lr, lam, epoch_count):\n",
        "  \"\"\"\n",
        "  :param X_train: Feature data for learning\n",
        "  :param Y_train: label data for learning\n",
        "  :param lr: learning rate\n",
        "  :param lam: lambda value which is control parameter for regularization\n",
        "  :param epoch_count: num of epoch\n",
        "  :return: (learning history, final theta)\n",
        "  \"\"\"\n",
        "\n",
        "  theta = np.sqrt(1 / X_train.shape[0]) * np.random.randn(1, X_train.shape[1]) # xavier initialization\n",
        "  history = {\n",
        "    'theta': np.zeros((epoch_count, theta.shape[1])),\n",
        "    'train_err': np.zeros(epoch_count),\n",
        "    'train_acc': np.zeros(epoch_count)\n",
        "  }\n",
        "\n",
        "  for epoch in range(epoch_count):\n",
        "    # calculate training error\n",
        "    Y_hat = logistic(theta, X_train)\n",
        "    train_err = objective(Y_hat, Y_train, lam, theta)\n",
        "    train_acc = accuracy(Y_hat, Y_train)\n",
        "\n",
        "    # log history\n",
        "    history['theta'][epoch] = np.squeeze(theta)\n",
        "    history['train_err'][epoch] = train_err\n",
        "    history['train_acc'][epoch] = train_acc\n",
        "\n",
        "    # gradient descent\n",
        "    grad_theta = gradient(X_train, Y_hat, Y_train, lam, theta)\n",
        "    theta -= lr * grad_theta\n",
        "\n",
        "  return history, theta\n",
        "\n",
        "lr = 0.7\n",
        "epoch_count = 200000\n",
        "\n",
        "# over-fitting\n",
        "over_fitting_lam = 0.000001\n",
        "history, over_fitting_theta = fit_logistic_regression_model(X_data, Y_data, lr, over_fitting_lam, epoch_count)\n",
        "\n",
        "# just-right\n",
        "just_right_lam = 0.001\n",
        "just_right_history, just_right_theta = fit_logistic_regression_model(X_data, Y_data, lr, just_right_lam, epoch_count)\n",
        "\n",
        "# under-fitting\n",
        "under_fitting_lam = 0.1\n",
        "under_fitting_history, under_fitting_theta = fit_logistic_regression_model(X_data, Y_data, lr, under_fitting_lam, epoch_count)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}