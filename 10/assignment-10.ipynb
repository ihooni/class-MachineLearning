{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "131BCnhVcBXNWAArrOS5PJ3Jbh6TZq3qO",
      "authorship_tag": "ABX9TyNyw0ve8C6CY/57l6CEl/00"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPTs6NGvh0pk",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5embDCreh1H9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "DATA_PATH = './mnist.csv'\n",
        "SIZE_ROW = 28\n",
        "SIZE_COL = 28\n",
        "\n",
        "# load data from file\n",
        "data = torch.from_numpy(\n",
        "    np.genfromtxt(DATA_PATH, delimiter=',')\n",
        ")\n",
        "\n",
        "# separate pixel values from labels\n",
        "X_raw = data[:, 1:]\n",
        "Y_raw = data[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lVNcpk7h7QJ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUlQaemTh8zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize X data\n",
        "X_data = (X_raw - torch.min(X_raw)) / (torch.max(X_raw) - torch.min(X_raw))\n",
        "\n",
        "# one-hot encoding Y data\n",
        "Y_data = Y_raw.to(torch.long)\n",
        "Y_data = torch.eye(torch.unique(Y_data).shape[0])[Y_data]\n",
        "\n",
        "# train data\n",
        "X_train = X_data[:1000].to(device)\n",
        "Y_train = Y_data[:1000].to(device)\n",
        "\n",
        "# test data\n",
        "X_test = X_data[1000:].to(device)\n",
        "Y_test = Y_data[1000:].to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXzfqS4ie5S",
        "colab_type": "text"
      },
      "source": [
        "## 3. Define model and functions for learning neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOC76Zyeigt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def initialize_weights(fan_in, fan_out):\n",
        "  # xavier initialization\n",
        "  return math.sqrt(1 / fan_in) * torch.randn(fan_out, fan_in)\n",
        "\n",
        "def activation(z):\n",
        "  return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "def objective(Y_pred, Y, lam=0, params=None):\n",
        "  epsilon = 1e-8\n",
        "  data_fidelity_term = (-1 / Y.shape[0]) * torch.sum(\n",
        "      Y * torch.log(Y_pred + epsilon) + (1 - Y) * torch.log(1 - Y_pred + epsilon)\n",
        "  )\n",
        "\n",
        "  decay_term = 0\n",
        "  if lam is not 0:\n",
        "    num_of_params = 0   # n\n",
        "    sum_of_params = 0   # sigma theta^2\n",
        "    for param in params.values():\n",
        "      num_of_params += param.shape[0] * param.shape[1]\n",
        "      sum_of_params += torch.sum(param ** 2)\n",
        "\n",
        "    decay_term = (lam / 2 * num_of_params) * sum_of_params\n",
        "\n",
        "  return data_fidelity_term + decay_term\n",
        "\n",
        "def accuracy(Y_pred, Y):\n",
        "  answer_Y_pred = torch.argmax(Y_pred, axis=1)\n",
        "  answer_Y = torch.argmax(Y, axis=1)\n",
        "  return torch.mean((answer_Y_pred == answer_Y).float())\n",
        "\n",
        "\n",
        "class FNN:\n",
        "  def __init__(self, layer_size_list):\n",
        "    self.num_of_layer = len(layer_size_list) - 1\n",
        "    self.params = dict()\n",
        "\n",
        "    # initialize params w and b (w is weights and b is biases)\n",
        "    for i in range(len(layer_size_list) - 1):\n",
        "      fan_in = layer_size_list[i]\n",
        "      fan_out = layer_size_list[i + 1]\n",
        "\n",
        "      self.params['w' + str(i + 1)] = initialize_weights(fan_in, fan_out).double().to(device)\n",
        "      self.params['b' + str(i + 1)] = torch.zeros(1, fan_out).double().to(device)\n",
        "\n",
        "  def forward(self, X):\n",
        "    forward_results = dict()\n",
        "    forward_results['a0'] = X\n",
        "\n",
        "    # z is fully connected layer's result and\n",
        "    # a is activation function's result\n",
        "    for i in range(self.num_of_layer):\n",
        "      prev_a = forward_results['a' + str(i)]\n",
        "      curr_w = self.params['w' + str(i + 1)]\n",
        "      curr_b = self.params['b' + str(i + 1)]\n",
        "\n",
        "      forward_results['z' + str(i + 1)] = torch.matmul(prev_a, curr_w.T) + curr_b\n",
        "      forward_results['a' + str(i + 1)] = activation(curr_z)\n",
        "\n",
        "    forward_results['Y_pred'] = forward_results['a' + str(self.num_of_layer)]\n",
        "\n",
        "    return forward_results\n",
        "\n",
        "  def backward(self, X, Y, forward_results, lam=0):\n",
        "    grads = dict()\n",
        "\n",
        "    dz_last = (forward_results['a' + str(self.num_of_layer)] - Y) / X.shape[0]\n",
        "\n",
        "    # reverse order because of `back`ward propagation\n",
        "    for i in range(self.num_of_layer)[::-1]:\n",
        "      curr_a = forward_results['a' + str(i + 1)]\n",
        "      next_a = forward_results['a' + str(i)]\n",
        "\n",
        "      if i == (self.num_of_layer - 1):  # if last layer\n",
        "        grads['z' + str(i + 1)] = (curr_a - Y) / X.shape[0]\n",
        "      else:\n",
        "        prev_dz = grads['z' + str(i + 2)]\n",
        "        prev_w = self.params['w' + str(i + 2)]\n",
        "\n",
        "        grads['a' + str(i + 1)] = torch.matmul(prev_dz, prev_w)\n",
        "        grads['z' + str(i + 1)] = curr_a * (1 - curr_a) * grads['a' + str(i + 1)]\n",
        "\n",
        "      grads['w' + str(i + 1)] = torch.matmul(grads['z' + str(i + 1)].T, next_a)\n",
        "      grads['b' + str(i + 1)] = torch.sum(grads['z' + str(i + 1)], axis=0).reshape(1, -1)\n",
        "\n",
        "    # remain only the gradients of w and b\n",
        "    for key in grads.keys():\n",
        "      if key.startswith('z') or key.startswith('a'):\n",
        "        del grads[key]\n",
        "\n",
        "    # apply gradients of decay term\n",
        "    if lam is not 0:\n",
        "      n = 0\n",
        "      for param in self.params.values():\n",
        "        n += param.shape[0] * param.shape[1]\n",
        "\n",
        "      for key in grads.keys():\n",
        "        grads[key] += (lam * n) * self.params[key]\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}