{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1VSPPe_Ym989M6_6dg1l8ir0ULJ7VFVLV",
      "authorship_tag": "ABX9TyOB7FOCOECKw0e8XHZh8ExD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ_z7BD8hdBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ./drive/My\\ Drive/Colab\\ Notebooks/class-MachineLearning/11/movie_review.zip ./\n",
        "!unzip ./movie_review.zip\n",
        "!rm -rf ./movie_review/Icon ./movie_review/pos/Icon ./movie_review/neg/Icon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwbZ1sHShNYf",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load data from files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s1CKaBQhOv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "\n",
        "DATA_PATH = './movie_review'\n",
        "\n",
        "data = load_files(DATA_PATH)\n",
        "X_raw, Y_raw = data.data, data.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKtDl8S2VTuQ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3OCLYzXyyu3",
        "colab_type": "text"
      },
      "source": [
        "### 2-1. Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLE9K98TVY0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "docs = []\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "# text preprocssing\n",
        "for doc_raw in X_raw:\n",
        "  # byte to string\n",
        "  doc = doc_raw.decode('utf-8')\n",
        "  \n",
        "  # remove all the special characters\n",
        "  doc = re.sub(r'\\W', ' ', doc)\n",
        "\n",
        "  # remove all single characters\n",
        "  doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n",
        "  doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc)\n",
        "\n",
        "  # substitute multiple spaces with single space\n",
        "  doc = re.sub(r'\\s+', ' ', doc, flags=re.I)\n",
        "\n",
        "  # convert to lowercase\n",
        "  doc = doc.lower()\n",
        "\n",
        "  # lemmatization\n",
        "  word_list = doc.split()\n",
        "  lemma_word_list = []\n",
        "  for word, tag in pos_tag(word_list):\n",
        "    main_tag = tag[0].lower()\n",
        "    lemma_word = ''\n",
        "\n",
        "    if main_tag in ['a', 'r', 'n', 'v']:\n",
        "      lemma_word = stemmer.lemmatize(word, main_tag)\n",
        "    else:\n",
        "      lemma_word = word\n",
        "\n",
        "    lemma_word_list.append(lemma_word)\n",
        "\n",
        "  doc = ' '.join(lemma_word_list)\n",
        "  docs.append(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HgTmPIuy5bc",
        "colab_type": "text"
      },
      "source": [
        "### 2-2. Documents to vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOuMR6eIy94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# documents to tfidf vector\n",
        "vectorizer = CountVectorizer(max_features=1600, min_df=0.01, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X_data = vectorizer.fit_transform(docs).toarray()\n",
        "tfidf_transformer = TfidfTransformer(sublinear_tf=False)\n",
        "X_data = tfidf_transformer.fit_transform(X_data).toarray()\n",
        "\n",
        "Y_data = Y_raw\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.3, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}