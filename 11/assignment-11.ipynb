{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1VSPPe_Ym989M6_6dg1l8ir0ULJ7VFVLV",
      "authorship_tag": "ABX9TyO+Mgf+QM2r9hH+F/5e9Ywx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ_z7BD8hdBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ./drive/My\\ Drive/Colab\\ Notebooks/class-MachineLearning/11/movie_review.zip ./\n",
        "!unzip ./movie_review.zip\n",
        "!rm -rf ./movie_review/Icon ./movie_review/pos/Icon ./movie_review/neg/Icon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwbZ1sHShNYf",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load data from files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s1CKaBQhOv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "\n",
        "DATA_PATH = './movie_review'\n",
        "\n",
        "data = load_files(DATA_PATH)\n",
        "X_raw, Y_raw = data.data, data.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKtDl8S2VTuQ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3OCLYzXyyu3",
        "colab_type": "text"
      },
      "source": [
        "### 2-1. Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLE9K98TVY0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "docs = []\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "# text preprocssing\n",
        "for doc_raw in X_raw:\n",
        "  # byte to string\n",
        "  doc = doc_raw.decode('utf-8')\n",
        "  \n",
        "  # remove all the special characters\n",
        "  doc = re.sub(r'\\W', ' ', doc)\n",
        "\n",
        "  # remove all single characters\n",
        "  doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n",
        "  doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc)\n",
        "\n",
        "  # substitute multiple spaces with single space\n",
        "  doc = re.sub(r'\\s+', ' ', doc, flags=re.I)\n",
        "\n",
        "  # convert to lowercase\n",
        "  doc = doc.lower()\n",
        "\n",
        "  # lemmatization\n",
        "  word_list = doc.split()\n",
        "  lemma_word_list = []\n",
        "  for word, tag in pos_tag(word_list):\n",
        "    main_tag = tag[0].lower()\n",
        "    lemma_word = ''\n",
        "\n",
        "    if main_tag in ['a', 'r', 'n', 'v']:\n",
        "      lemma_word = stemmer.lemmatize(word, main_tag)\n",
        "    else:\n",
        "      lemma_word = word\n",
        "\n",
        "    lemma_word_list.append(lemma_word)\n",
        "\n",
        "  doc = ' '.join(lemma_word_list)\n",
        "  docs.append(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HgTmPIuy5bc",
        "colab_type": "text"
      },
      "source": [
        "### 2-2. Documents to vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOuMR6eIy94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# documents to tfidf vector\n",
        "vectorizer = CountVectorizer(max_features=1600, min_df=0.01, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X_data = vectorizer.fit_transform(docs).toarray()\n",
        "tfidf_transformer = TfidfTransformer(sublinear_tf=False)\n",
        "X_data = tfidf_transformer.fit_transform(X_data).toarray()\n",
        "\n",
        "Y_data = Y_raw\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.3, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEZp1yz24QFU",
        "colab_type": "text"
      },
      "source": [
        "### 2-3. Move data to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWM7cKlq4V05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "X_train = torch.from_numpy(X_train).to(device)\n",
        "Y_train = torch.from_numpy(Y_train).to(device)\n",
        "\n",
        "X_test = torch.from_numpy(X_test).to(device)\n",
        "Y_test = torch.from_numpy(Y_test).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5AnH8326UQl",
        "colab_type": "text"
      },
      "source": [
        "## 3. Define model and functions for learning neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrWLkdYf6VRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def initialize_weights(fan_in, fan_out):\n",
        "  # xavier initialization\n",
        "  return math.sqrt(1 / fan_in) * torch.randn(fan_out, fan_in)\n",
        "\n",
        "def activation(z):\n",
        "  return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "def objective(Y_pred, Y, lam=0, params=None):\n",
        "  epsilon = 1e-8\n",
        "  data_fidelity_term = (-1 / Y.shape[0]) * torch.sum(\n",
        "      Y * torch.log(Y_pred + epsilon) + (1 - Y) * torch.log(1 - Y_pred + epsilon)\n",
        "  )\n",
        "\n",
        "  decay_term = 0\n",
        "  if lam is not 0:\n",
        "    num_of_params = 0   # n\n",
        "    sum_of_params = 0   # sigma theta^2\n",
        "    for param in params.values():\n",
        "      num_of_params += param.shape[0] * param.shape[1]\n",
        "      sum_of_params += torch.sum(param ** 2)\n",
        "\n",
        "    decay_term = (lam / (2 * num_of_params)) * sum_of_params\n",
        "\n",
        "  return data_fidelity_term + decay_term\n",
        "\n",
        "def accuracy(Y_pred, Y):\n",
        "  answer = (Y_pred >= 0.5).float()\n",
        "  return torch.mean((answer == Y).float())\n",
        "\n",
        "\n",
        "class FNN:\n",
        "  def __init__(self, layer_size_list):\n",
        "    self.num_of_layer = len(layer_size_list) - 1\n",
        "    self.params = dict()\n",
        "\n",
        "    # initialize params w and b (w is weights and b is biases)\n",
        "    for i in range(len(layer_size_list) - 1):\n",
        "      fan_in = layer_size_list[i]\n",
        "      fan_out = layer_size_list[i + 1]\n",
        "\n",
        "      self.params['w' + str(i + 1)] = initialize_weights(fan_in, fan_out).double().to(device)\n",
        "      self.params['b' + str(i + 1)] = torch.zeros(1, fan_out).double().to(device)\n",
        "\n",
        "  def forward(self, X):\n",
        "    forward_results = dict()\n",
        "    forward_results['a0'] = X\n",
        "\n",
        "    # z is fully connected layer's result and\n",
        "    # a is activation function's result\n",
        "    for i in range(self.num_of_layer):\n",
        "      prev_a = forward_results['a' + str(i)]\n",
        "      curr_w = self.params['w' + str(i + 1)]\n",
        "      curr_b = self.params['b' + str(i + 1)]\n",
        "\n",
        "      forward_results['z' + str(i + 1)] = torch.matmul(prev_a, curr_w.T) + curr_b\n",
        "      forward_results['a' + str(i + 1)] = activation(forward_results['z' + str(i + 1)])\n",
        "\n",
        "    forward_results['Y_pred'] = forward_results['a' + str(self.num_of_layer)]\n",
        "\n",
        "    return forward_results\n",
        "\n",
        "  def backward(self, X, Y, forward_results, lam=0):\n",
        "    grads = dict()\n",
        "\n",
        "    dz_last = (forward_results['a' + str(self.num_of_layer)] - Y) / X.shape[0]\n",
        "\n",
        "    # reverse order because of `back`ward propagation\n",
        "    for i in range(self.num_of_layer)[::-1]:\n",
        "      curr_a = forward_results['a' + str(i + 1)]\n",
        "      next_a = forward_results['a' + str(i)]\n",
        "\n",
        "      if i == (self.num_of_layer - 1):  # if last layer\n",
        "        grads['z' + str(i + 1)] = (curr_a - Y) / X.shape[0]\n",
        "      else:\n",
        "        prev_dz = grads['z' + str(i + 2)]\n",
        "        prev_w = self.params['w' + str(i + 2)]\n",
        "\n",
        "        grads['a' + str(i + 1)] = torch.matmul(prev_dz, prev_w)\n",
        "        grads['z' + str(i + 1)] = curr_a * (1 - curr_a) * grads['a' + str(i + 1)]\n",
        "\n",
        "      grads['w' + str(i + 1)] = torch.matmul(grads['z' + str(i + 1)].T, next_a)\n",
        "      grads['b' + str(i + 1)] = torch.sum(grads['z' + str(i + 1)], axis=0).reshape(1, -1)\n",
        "\n",
        "    # remain only the gradients of w and b\n",
        "    for key in list(grads.keys()):\n",
        "      if key.startswith('z') or key.startswith('a'):\n",
        "        del grads[key]\n",
        "\n",
        "    # apply gradients of decay term\n",
        "    if lam is not 0:\n",
        "      n = 0\n",
        "      for param in self.params.values():\n",
        "        n += param.shape[0] * param.shape[1]\n",
        "\n",
        "      for key in grads.keys():\n",
        "        grads[key] += (lam / n) * self.params[key]\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}