{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pASv_zLmwefcOCUSgubW9pViVnXavK3w",
      "authorship_tag": "ABX9TyNpCoI812QjBB+zHhzL5HyS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT5qoVz-98wo",
        "colab_type": "text"
      },
      "source": [
        "### 1. Load data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9thwELZ99wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "DATA_PATH = './data-nonlinear.txt'\n",
        "\n",
        "# load data from file\n",
        "data = np.genfromtxt(DATA_PATH, delimiter=',')\n",
        "\n",
        "# separate features from labels\n",
        "X_raw = data[:, 0:-1]\n",
        "Y_raw = data[:, -1].reshape(-1, 1)\n",
        "\n",
        "# prepare data for learning\n",
        "X_data = np.concatenate(\n",
        "    (\n",
        "        np.ones((len(X_raw), 1)),                     # 1\n",
        "        X_raw,                                        # x and y\n",
        "        (X_raw[:, 0] * X_raw[:, 1]).reshape(-1, 1),   # xy\n",
        "        X_raw ** 2                                    # x^2 and y^2\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "Y_data = Y_raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28IEHulQCXkA",
        "colab_type": "text"
      },
      "source": [
        "### 2. Define functions for logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucZKoUCxCYkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic(theta, X):\n",
        "  z = np.matmul(X, theta.T)\n",
        "  Y_hat = 1 / (1 + np.exp(-z))\n",
        "  return Y_hat\n",
        "\n",
        "def objective(Y_hat, Y):\n",
        "  epsilon = 1e-8\n",
        "  return -1 * np.mean(\n",
        "      Y * np.log(Y_hat + epsilon) + (1 - Y) * np.log(1 - Y_hat + epsilon), axis=0\n",
        "  )\n",
        "\n",
        "def gradient(X, Y_hat, Y):\n",
        "  m = len(X)\n",
        "  return (1 / m) * np.matmul((Y_hat - Y).T, X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFLfjE7KIdWi",
        "colab_type": "text"
      },
      "source": [
        "### 3. Learning with the gradient descent algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W0WvbCHId9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta = np.sqrt(1 / X_data.shape[0]) * np.random.randn(1, X_data.shape[1]) # xavier initialization\n",
        "lr = 0.5\n",
        "epoch_count = 5000\n",
        "\n",
        "history = {\n",
        "    'theta': np.zeros((epoch_count, theta.shape[1])),\n",
        "    'train_err': np.zeros(epoch_count)\n",
        "}\n",
        "\n",
        "for epoch in range(epoch_count):\n",
        "  # calculate training error\n",
        "  Y_hat = logistic(theta, X_data)\n",
        "  train_err = objective(Y_hat, Y_data)\n",
        "\n",
        "  # log history\n",
        "  history['theta'][epoch] = np.squeeze(theta)\n",
        "  history['train_err'][epoch] = train_err\n",
        "\n",
        "  # gradient descent\n",
        "  grad_theta = gradient(X_data, Y_hat, Y_data)\n",
        "  theta -= lr * grad_theta"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}